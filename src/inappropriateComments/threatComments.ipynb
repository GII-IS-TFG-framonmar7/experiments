{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#84b6f4;\">Detección de comentarios de hate</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#77dd77;\">Formación del DataFrame</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, sacamos los datos del fichero txt y formamos un DataFrame con ellos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  has_threat\n",
      "0  Un poco solitario aquí no parece tener muchos ...           0\n",
      "1  @ManuelViloria ¡Gracias!Estoy un poco asustado...           0\n",
      "2  txt chat con Jake lmfao it frikkinawesomei ech...           0\n",
      "3  Soy un estudiante de primer año en la universi...           0\n",
      "4  espera montón de videos aprender idiomas sueño...           0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Función para leer y transformar un archivo Parquet.\n",
    "def read_and_transform(parquet_path):\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    columns_of_interest = [col for col in df.columns if col == 'text' or col == 'has_threat']\n",
    "    df = df[columns_of_interest]\n",
    "    return df\n",
    "\n",
    "# Especificamos los archivos Parquet utilizando os.path.join.\n",
    "parquet_files = ['0000.parquet', '0001.parquet', '0002.parquet']\n",
    "data_frames = []\n",
    "\n",
    "for file_name in parquet_files:\n",
    "    file_path = os.path.join(os.getcwd(), 'resources', file_name)\n",
    "    data_frames.append(read_and_transform(file_path))\n",
    "\n",
    "# Concatenamos todos los DataFrames.\n",
    "concatenated_data_frame = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "# Mostramos las primeras filas del DataFrame balanceado.\n",
    "print(concatenated_data_frame.head(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#77dd77;\">Entrenamiento</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionamos las métricas que usaremos para predecir, así como el atributo objetivo que, en este caso, será numérico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtramos el DataFrame para obtener las muestras con has_threat = 1\n",
    "threat_df = concatenated_data_frame[concatenated_data_frame['has_threat'] == 1]\n",
    "\n",
    "# Filtramos el DataFrame para obtener las muestras con has_threat = 0\n",
    "non_threat_df = concatenated_data_frame[concatenated_data_frame['has_threat'] == 0].sample(n=1000, random_state=42)\n",
    "\n",
    "# Concatenamos los DataFrames balanceados\n",
    "balanced_df = pd.concat([threat_df, non_threat_df], ignore_index=True)\n",
    "\n",
    "# Mezclamos las muestras para asegurar un orden aleatorio\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Atributo que usaremos para predecir\n",
    "text = balanced_df['text']\n",
    "\n",
    "# Atributo objetivo a predecir\n",
    "goal = balanced_df['has_threat']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, entrenamos un modelo de clasificación con una red neuronal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo en el conjunto de prueba: 0.7261698440207972\n",
      "F1-Score del modelo: 0.7322033898305085\n",
      "[[203  98]\n",
      " [ 60 216]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento (70%) y prueba (30%)\n",
    "attributes_train, attributes_test, goal_train, goal_test = train_test_split(text, goal, test_size=0.3, random_state=42)\n",
    "\n",
    "# Creamos un vectorizador de texto\n",
    "vectorizer = CountVectorizer()\n",
    "attributes_train_vectorized = vectorizer.fit_transform(attributes_train)\n",
    "attributes_test_vectorized = vectorizer.transform(attributes_test)\n",
    "\n",
    "# Entrenamos el modelo de clasificación de redes neuronales\n",
    "threat_classifier = MLPClassifier(hidden_layer_sizes=(50, 50, 50), activation='relu', solver='adam', random_state=42, max_iter=1000)\n",
    "threat_classifier.fit(attributes_train_vectorized, goal_train)\n",
    "\n",
    "# Realizamos predicciones con el conjunto de prueba\n",
    "prediction = threat_classifier.predict(attributes_test_vectorized)\n",
    "\n",
    "# Calculamos la precisión y F1-Score del modelo\n",
    "accuracy = accuracy_score(goal_test, prediction)\n",
    "f1 = f1_score(goal_test, prediction)\n",
    "print(\"Precisión del modelo en el conjunto de prueba:\", accuracy)\n",
    "print(\"F1-Score del modelo:\", f1)\n",
    "\n",
    "# Mostramos la matriz de confusión\n",
    "print(confusion_matrix(goal_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos el modelo con algunos ejemplos concretos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El comentario 'Me encanta la peli.' es una amenaza.\n",
      "El comentario 'Iremos a buscarte' es una amenaza.\n",
      "El comentario 'Te felicito por tu trabajo; sin embargo, tienes que mejorar tu oratoria.' NO es una amenaza.\n",
      "El comentario 'Te voy a matar.' es una amenaza.\n",
      "El comentario 'Te quiero, pero eres una basura.' NO es una amenaza.\n"
     ]
    }
   ],
   "source": [
    "message = [\"NO es una amenaza\", \"es una amenaza\"]\n",
    "\n",
    "# Texto de ejemplo 1\n",
    "example_text_1 = \"Me encanta la peli.\"\n",
    "example_text_1_vectorized = vectorizer.transform([example_text_1])\n",
    "\n",
    "# Texto de ejemplo 2\n",
    "example_text_2 = \"Iremos a buscarte\"\n",
    "example_text_2_vectorized = vectorizer.transform([example_text_2])\n",
    "\n",
    "# Texto de ejemplo 3\n",
    "example_text_3 = \"Te felicito por tu trabajo; sin embargo, tienes que mejorar tu oratoria.\"\n",
    "example_text_3_vectorized = vectorizer.transform([example_text_3])\n",
    "\n",
    "# Texto de ejemplo 4\n",
    "example_text_4 = \"Te voy a matar.\"\n",
    "example_text_4_vectorized = vectorizer.transform([example_text_4])\n",
    "\n",
    "# Texto de ejemplo 5\n",
    "example_text_5 = \"Te quiero, pero eres una basura.\"\n",
    "example_text_5_vectorized = vectorizer.transform([example_text_5])\n",
    "\n",
    "print(\"El comentario '\" + example_text_1 + \"' \" + message[threat_classifier.predict(example_text_1_vectorized)[0]] + \".\")\n",
    "print(\"El comentario '\" + example_text_2 + \"' \" + message[threat_classifier.predict(example_text_2_vectorized)[0]] + \".\")\n",
    "print(\"El comentario '\" + example_text_3 + \"' \" + message[threat_classifier.predict(example_text_3_vectorized)[0]] + \".\")\n",
    "print(\"El comentario '\" + example_text_4 + \"' \" + message[threat_classifier.predict(example_text_4_vectorized)[0]] + \".\")\n",
    "print(\"El comentario '\" + example_text_5 + \"' \" + message[threat_classifier.predict(example_text_5_vectorized)[0]] + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para finalizar, exportamos el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['threat_vectorizer.joblib']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "# Guardamos el modelo\n",
    "model_filename = 'threat_classifier.joblib'\n",
    "dump(threat_classifier, model_filename)\n",
    "\n",
    "# Guardamos el vectorizador\n",
    "vectorizer_filename = 'threat_vectorizer.joblib'\n",
    "dump(vectorizer, vectorizer_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
