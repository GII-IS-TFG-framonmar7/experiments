{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#84b6f4;\">Detección de comentarios de hate</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#77dd77;\">Formación del DataFrame</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, sacamos los datos del fichero txt y formamos un DataFrame con ellos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\fjmon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\fjmon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      id                                               text  \\\n",
      "0  id=828025263321657348  Ismael es egocentrico porque se vuelve loca si...   \n",
      "1  id=828025128797741057  ..ya tardaba en salir quien pronunciase nombre...   \n",
      "2  id=828025087815274496  (Esto no es un discurso político y razonado, o...   \n",
      "3  id=828025006626058241  Muy despreciados,siiii,pero todos vestidos de ...   \n",
      "4  id=828024709761658880  marica explicame porque a veces no te entiendo...   \n",
      "\n",
      "   hate  \n",
      "0     0  \n",
      "1     0  \n",
      "2     0  \n",
      "3     1  \n",
      "4     1  \n",
      "4406\n",
      "4674\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return list(synonyms)\n",
    "\n",
    "def synonym_replacement(sentence, n):\n",
    "    words = sentence.split()\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([word for word in words if wordnet.synsets(word)]))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(list(synonyms))\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n: # n define el número de palabras a reemplazar\n",
    "            break\n",
    "\n",
    "    sentence = ' '.join(new_words)\n",
    "    return sentence\n",
    "\n",
    "# Especificamos el archivo de texto\n",
    "txt_file = os.getcwd()+'/resources/labeled_corpus_6K.txt'\n",
    "\n",
    "# Leemos el archivo de texto con el formato específico\n",
    "data = []\n",
    "with open(txt_file, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split(';||;')\n",
    "        if len(parts) == 3:\n",
    "            data.append(parts)\n",
    "\n",
    "# Creamos un DataFrame a partir de los datos\n",
    "column_names = ['id', 'text', 'hate']\n",
    "data_frame = pd.DataFrame(data, columns=column_names)\n",
    "\n",
    "# Convertimos la columna 'hate' en enteros\n",
    "data_frame['hate'] = data_frame['hate'].astype(int)\n",
    "\n",
    "# Seleccionamos las filas con 'hate=0'\n",
    "hate_1_rows = data_frame[data_frame['hate'] == 1].copy()\n",
    "\n",
    "# Generamos filas aumentadas\n",
    "augmented_rows = []\n",
    "for index, row in hate_1_rows.iterrows():\n",
    "    new_text_1 = synonym_replacement(row['text'], 2)\n",
    "    augmented_row_1 = [row['id'], new_text_1, row['hate']]\n",
    "    new_text_2 = synonym_replacement(row['text'], 2)\n",
    "    augmented_row_2 = [row['id'], new_text_2, row['hate']]\n",
    "\n",
    "    if new_text_1 != new_text_2:\n",
    "        augmented_rows.append(augmented_row_1)\n",
    "        augmented_rows.append(augmented_row_2)\n",
    "    else:\n",
    "        augmented_rows.append(augmented_row_1)\n",
    "\n",
    "# Creamos un DataFrame con las filas aumentadas\n",
    "augmented_data_frame = pd.DataFrame(augmented_rows, columns=column_names)\n",
    "\n",
    "# Concatenamos el DataFrame original con las filas aumentadas\n",
    "balanced_data_frame = pd.concat([data_frame, augmented_data_frame])\n",
    "\n",
    "# Mostramos las primeras filas del DataFrame aumentado y balanceado\n",
    "print(balanced_data_frame.head(5))\n",
    "\n",
    "print(balanced_data_frame[balanced_data_frame['hate'] == 0].shape[0])\n",
    "print(balanced_data_frame[balanced_data_frame['hate'] == 1].shape[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#77dd77;\">Entrenamiento</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionamos las métricas que usaremos para predecir, así como el atributo objetivo que, en este caso, será numérico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atributo que usaremos para predecir\n",
    "attributes = balanced_data_frame[['text']]\n",
    "\n",
    "# Atributo objetivo a predecir\n",
    "goal = balanced_data_frame['hate']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, entrenamos un modelo de clasificación con una red neuronal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo en el conjunto de prueba: 0.9030837004405287\n",
      "F1-Score: 0.9085239085239085\n",
      "[[1149  196]\n",
      " [  68 1311]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "text = balanced_data_frame['text']\n",
    "\n",
    "# Dividimos los datos en conjuntos de entrenamiento (70%) y prueba (30%)\n",
    "attributes_train, attributes_test, goal_train, goal_test = train_test_split(text, goal, test_size=0.3, random_state=42)\n",
    "\n",
    "# Creamos un vectorizador de texto\n",
    "vectorizer = CountVectorizer()\n",
    "tweet_train_vectorized = vectorizer.fit_transform(attributes_train)\n",
    "tweet_test_vectorized = vectorizer.transform(attributes_test)\n",
    "\n",
    "# Entrenamos el modelo de clasificación de redes neuronales\n",
    "mlp_classifier = MLPClassifier(hidden_layer_sizes=(50, 50, 50), activation='relu', solver='adam', random_state=42, max_iter=1000)\n",
    "mlp_classifier.fit(tweet_train_vectorized, goal_train)\n",
    "\n",
    "# Realizamos predicciones con el conjunto de prueba\n",
    "prediction = mlp_classifier.predict(tweet_test_vectorized)\n",
    "\n",
    "# Calculamos la precisión y F1-Score del modelo\n",
    "accuracy = accuracy_score(goal_test, prediction)\n",
    "f1 = f1_score(goal_test, prediction)\n",
    "print(\"Precisión del modelo en el conjunto de prueba:\", accuracy)\n",
    "print(\"F1-Score:\", f1)\n",
    "\n",
    "# Mostramos la matriz de confusión\n",
    "print(confusion_matrix(goal_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos el modelo con algunos ejemplos concretos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El comentario 'Vaya que eres cortito, ¿eh?' es respetuoso.\n",
      "El comentario 'Es la peor película que he visto en los últimos años.' es respetuoso.\n",
      "El comentario 'Si te gusta esta peli es que eres de lo más tonto.' es de hate.\n",
      "El comentario 'Te aprecio mucho.' es respetuoso.\n",
      "El comentario 'Te quiero, pero eres una basura.' es de hate.\n"
     ]
    }
   ],
   "source": [
    "message = [\"respetuoso\", \"de hate\"]\n",
    "\n",
    "# Texto de ejemplo 1\n",
    "example_text_1 = \"Vaya que eres cortito, ¿eh?\"\n",
    "example_text_1_vectorized = vectorizer.transform([example_text_1])\n",
    "\n",
    "# Texto de ejemplo 2\n",
    "example_text_2 = \"Es la peor película que he visto en los últimos años.\"\n",
    "example_text_2_vectorized = vectorizer.transform([example_text_2])\n",
    "\n",
    "# Texto de ejemplo 3\n",
    "example_text_3 = \"Si te gusta esta peli es que eres de lo más tonto.\"\n",
    "example_text_3_vectorized = vectorizer.transform([example_text_3])\n",
    "\n",
    "# Texto de ejemplo 4\n",
    "example_text_4 = \"Te aprecio mucho.\"\n",
    "example_text_4_vectorized = vectorizer.transform([example_text_4])\n",
    "\n",
    "# Texto de ejemplo 5\n",
    "example_text_5 = \"Te quiero, pero eres una basura.\"\n",
    "example_text_5_vectorized = vectorizer.transform([example_text_5])\n",
    "\n",
    "print(\"El comentario '\" + example_text_1 + \"' es \" + message[mlp_classifier.predict(example_text_1_vectorized)[0]] + \".\")\n",
    "print(\"El comentario '\" + example_text_2 + \"' es \" + message[mlp_classifier.predict(example_text_2_vectorized)[0]] + \".\")\n",
    "print(\"El comentario '\" + example_text_3 + \"' es \" + message[mlp_classifier.predict(example_text_3_vectorized)[0]] + \".\")\n",
    "print(\"El comentario '\" + example_text_4 + \"' es \" + message[mlp_classifier.predict(example_text_4_vectorized)[0]] + \".\")\n",
    "print(\"El comentario '\" + example_text_5 + \"' es \" + message[mlp_classifier.predict(example_text_5_vectorized)[0]] + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para finalizar, exportamos el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hate_vectorizer.joblib']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "# Guardamos el modelo\n",
    "model_filename = 'hate_classifier.joblib'\n",
    "dump(mlp_classifier, model_filename)\n",
    "\n",
    "# Guardamos el vectorizador\n",
    "vectorizer_filename = 'hate_vectorizer.joblib'\n",
    "dump(vectorizer, vectorizer_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
